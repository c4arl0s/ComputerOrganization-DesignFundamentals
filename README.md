# Computer Organization and Design Fundamentals

1. [Chapter One: Digital Signals and Systems](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#1-chapter-one-digital-signals-and-systems-1)
2. [Chapter Two: Numbering Systems]()
3. [Chapter Three: Binary Math and Signed Representations]()
4. [Chapter Four: Logic Functions and Gates]()
5. [Chapter Five: Boolean Algebra]()
6. [Chapter Six: Standard Boolean Expression Formats]()
7. [Chapter Seven: Karnaugh Maps]()
8. [Chapter Eight: Combinational Logic Applications]()
9. [Chapter Nine: Binary Operation Applications]()
10. [Chapter Ten: Memory Cells]()
11. [Chapter Eleven: State Machines]()
12. [Chapter Twelve: Memory Organization]()
13. [Chapter Thirteen: Memory Hierarchy]()
14. [Chapter Fourteen: Serial Protocol Basics]()
15. [Chapter Fifteen: Introduction to Processor Architecture]()
16. [Chapter Sixteen: Intel 80x86 Base Architecture]()

# 1. [Chapter One: Digital Signals and Systems](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#computer-organization-and-design-fundamentals)
 * [1.1 Should Software Engineers Worry About Hardware?](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#-11-should-software-engineers-worry-about-hardware)
 * [1.2 Non-Digital Signal](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#-12-non-digital-signal)
 * [1.3 Digital Signals](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#-13-digital-signals)
 * [1.4 Conversion Systems](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#-14-conversion-systems)
 * [1.5 Representation of Digital Signals](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#-15-representation-of-digital-signals)
 * [1.6 Types of Digital Signals](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#-16-types-of-digital-signals)
 * [1.6.1 Edges](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#-161-edges)
 * [1.6.2 Pulses](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#-162-pulses)
 * [1.6.3 Non-Periodic Pulse Trains](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#-163-non-periodic-pulse-trains)
 * [1.6.4 Periodic Pulse Trains](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#-164-periodic-pulse-trains)
 * [1.6.5 Pulse-Width Modulation](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#-165-pulse-width-modulation)
 * [1.7 Unit Prefixes](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#-17-unit-prefixes)
 * [1.8 What's Next?](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#-18-whats-next)
 * [Problems](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#-problems)

# 2. [Chapter Two: Numbering Systems]()
 * [2.1 Unsigned Binary Counting]()
 * [2.2 Binary Terminology]()
 * [2.3 Unsigned Binary to Decimal Conversion]()
 * [2.4 Decimal to Unsigned Binary Conversion]()
 * [2.5 Binary Representation of Analog Values]()
 * [2.6 Sampling Theory]()
 * [2.7 Hexadecimal Representation]()
 * [2.8 Binary Coded Decimal]()
 * [2.9 Gray Codes]()
 * [2.10 What's Next?]()
 * [Problems]()

# 3. [Chapter Three: Binary Math and Signed Representations]()
 * [3.1 Binary Addition]()
 * [3.2 Binary Subtraction]()
 * [3.3 Binary Complements]()
 * [3.3.1 One's Complement]()
 * [3.3.2 Two's Complement]()
 * [3.3.3 Most Significant Bit as a Sign Indicator]()
 * [3.3.4 Signed Magnitude]()
 * [3.3.5 MSB and Number of Bits]()
 * [3.3.6 Issues Surrounding the Conversion of Binary Numbers. 52 ]()
 * [3.3.7 Minimums and Maximums]()
 * [3.4 Floating Point Binary]()
 * [3.5 Hexadecimal Addition]()
 * [3.6 BCD Addition]()
 * [3.7 Multiplication and Division by Powers of Two]()
 * [3.8 Easy Decimal to Binary Conversion Trick]()
 * [3.9 Arithmetic Overflow]()
 * [3.10 What's Next?]()
 * [Problems]()

# 4. [Chapter Four: Logic Functions and Gates]()
 * [4.1 Logic Gate Basics]()
 * [4.1.1 NOT Gate]()
 * [4.1.2 AND Gate]()
 * [4.1.3 OR Gate]()
 * [4.1.4 Exclusive-OR (XOR) Gate]()
 * [4.2 Truth Tables]()
 * [4.3 Timing Diagrams for Gates]()
 * [4.4 Combinational Logic]()
 * [4.5 Truth Tables for Combinational Logic]()
 * [4.6 What's Next?]()
 * [Problems]()

# 5. [Chapter Five: Boolean Algebra]()
 * [5.1 Need for Boolean Expressions]()
 * [5.2 Symbols of Boolean Algebra]()
 * [5.3 Boolean Expressions of Combinational Logic]()
 * [5.4 Laws of Boolean Algebra]()
 * [5.5 Rules of Boolean Algebra]()
 * [5.5.1 NOT Rule]()
 * [5.5.2 OR Rules]()
 * [5.5.3 AND Rules]()
 * [5.5.4 XOR Rules]()
 * [5.5.5 Derivation of Other Rules]()
 * [5.6 Simplification]()
 * [5.7 DeMorgan's Theorem]()
 * [5.8 What's Next?]()
 * [Problems]()

# 6. [Chapter Six: Standard Boolean Expression Formats]()
 * [6.1 Sum-of-Products]()
 * [6.2 Converting an SOP Expression to a Truth Table]()
 * [6.3 Converting a Truth Table to an SOP Expression]()
 * [6.4 Product-of-Sums]()
 * [6.5 Converting POS to Truth Table]()
 * [6.6 Converting a Truth Table to a POS Expression]()
 * [6.7 NAND-NAND Logic]()
 * [6.8 What's Next?]()
 * [Problems]()

# 7. [Chapter Seven: Karnaugh Maps]()
 * [7.1 The Karnaugh Map]()
 * [7.2 Using Karnaugh Maps]()
 * [7.3 "Don't Care" Conditions in a Karnaugh Map]()
 * [7.4 What's Next?]()
 * [Problems]()

# 8. [Chapter Eight: Combinational Logic Applications]()
 * [8.1 Adders]()
 * [8.2 Seven-Segment Displays]()
 * [8.3 Active-Low Signals]()
 * [8.4 Decoders]()
 * [8.5 Multiplexers]()
 * [8.6 Demultiplexers]()
 * [8.7 Integrated Circuits]()
 * [8.8 What's Next?]()
 * [Problems]()

# 9. [Chapter Nine: Binary Operation Applications]()
 * [9.1 Bitwise Operations]()
 * [9.2 Comparing Bits with XOR]()
 * [9.3 Parity]()
 * [9.4 Checksum]()
 * [9.5 Cyclic Redundancy Check]()
 * [9.5.1 CRC Process]()
 * [9.5.2 CRC Implementation]()
 * [9.6 Hamming Code]()
 * [9.7 What's Next?]()
 * [Problems]()

# 10.[ Chapter Ten: Memory Cells]()
 * [10.1 New Truth Table Symbols]()
 * [10.1.1 Edges/Transitions]()
 * [10.1.2 Previously Stored Values]()
 * [10.1.3 Undefined Values]()
 * [10.2 The S-R Latch]()
 * [10.3 The D Latch]()
 * [10.4 Divide-By-Two Circuit]()
 * [10.5 Counter]()
 * [10.6 Parallel Data Output]()
 * [10.7 What's Next?]()
 * [Problems]()

# 11.[ Chapter Eleven: State Machines]()
 * [11.1 Introduction to State Machines]()
 * [11.1.1 States]()
 * [11.1.2 State Diagrams]()
 * [11.1.3 Errors in State Diagrams]()
 * [11.1.4 Basic Circuit Organization]()
 * [11.2 State Machine Design Process]()
 * [11.3 Another State Machine Design: Pattern Detection]()
 * [11.4 Mealy Versus Moore State Machines]()
 * [11.5 What's Next?]()
 * [Problems]()

# 12.[ Chapter Twelve: Memory Organization]()
 * [12.1 Early Memory]()
 * [12.2 Organization of Memory Device]()
 * [12.3 Interfacing Memory to a Processor]()
 * [12.3.1 Buses]()
 * [12.3.2 Memory Maps]()
 * [12.3.3 Address Decoding]()
 * [12.3.4 Chip Select Hardware]()
 * [12.4 Memory Mapped Input/Output]()
 * [12.5 Memory Terminology]()
 * [12.5.1 Random Access Memory]()
 * [12.5.2 Read Only Memory]()
 * [12.5.3 Static RAM versus Dynamic RAM]()
 * [12.5.4 Types of DRAM and Their Timing]()
 * [12.5.5 Asynchronous vs. Synchronous Memory]()
 * [12.6 What's Next?]()
 * [Problems]()

# 13.[ Chapter Thirteen: Memory Hierarchy]()
 * [13.1 Characteristics of the Memory Hierarchy]()
 * [13.2 Physical Characteristics of a Hard Drive]()
 * [13.2.1 Hard Drive Read/Write Head]()
 * [13.2.2 Data Encoding]()
 * [13.2.3 Hard Drive Access Time]()
 * [13.2.4 S.M.A.R.T.]()
 * [13.3 Organization of Data on a Hard Drive]()
 * [13.4 Cache RAM]()
 * [13.4.1 Cache Organization]()
 * [13.4.2 Dividing Memory into Blocks]()
 * [13.4.3 Cache Operation]()
 * [13.4.4 Cache Characteristics]()
 * [13.4.5 Cache Mapping Functions]()
 * [13.4.6 Cache Write Policy]()
 * [13.5 Registers]()
 * [13.6 What's Next?]()
 * [Problems]()

# 14.[ Chapter Fourteen: Serial Protocol Basics]()
 * [14.1 OSI Seven-Layer Network Model]()
 * [14.2 Serial versus Parallel Data Transmission]()
 * [14.3 Anatomy of a Frame or Packet]()
 * [14.4 Sample Protocol: IEEE 802.3 Ethernet]()
 * [14.5 Sample Protocol: Internet Protocol]()
 * [14.6 Sample Protocol: Transmission Control Protocol]()
 * [14.7 Dissecting a Frame]()
 * [14.8 Additional Resources]()
 * [14.9 What's Next?]()
 * [Problems]()

# 15.[ Chapter Fifteen: Introduction to Processor Architecture]()
 * [15.1 Organization versus Architecture]()
 * [15.2 Components]()
 * [15.2.1 Bus]()
 * [15.2.2 Registers]()
 * [15.2.3 Flags]()
 * [15.2.4 Buffers]()
 * [15.2.5 The Stack]()
 * [15.2.6 I/O Ports]()
 * [15.3 Processor Level]()
 * [15.4 CPU Level]()
 * [15.5 Simple Example of CPU Operation]()
 * [15.6 Assembly and Machine Language]()
 * [15.7 Big-Endian/Little-Endian]()
 * [15.8 Pipelined Architectures]()
 * [15.9 Passing Data To and From Peripherals]()
 * [15.9.1 Memory-Mapped I/O]()
 * [15.9.2 Polling]()
 * [15.9.3 Interrupts]()
 * [15.9.4 Direct Memory Access]()
 * [15.9.5 I/O Channels and Processors]()
 * [15.10 What's Next?]()
 * [Problems]()

# 16.[ Chapter Sixteen: Intel 80x86 Base Architecture]()
 * [16.1 Why Study the 80x86?]()
 * [16.2 Execution Unit]()
 * [16.2.1 General Purpose Registers]()
 * [16.2.2 Address Registers]()
 * [16.2.3 Flags]()
 * [16.2.4 Internal Buses]()
 * [16.3 Bus Interface Unit]()
 * [16.3.1 Segment Addressing]()
 * [16.3.2 Instruction Queue]()
 * [16.4 Memory versus I/O Ports]()
 * [16.5 What's Next?]()
 * [Problems]()

# 17.[ Chapter Seventeen: Intel 80x86 Assembly Language]()
 * [17.1 Assemblers versus Compilers]()
 * [17.2 Components of a Line of Assembly Language]()
 * [17.3 Assembly Language Directives]()
 * [17.3.1 SEGMENT Directive]()
 * [17.3.2 .MODEL, .STACK, .DATA, and .CODE Directives . 380 ]()
 * [17.3.3 PROC Directive]()
 * [17.3.4 END Directive]()
 * [17.3.5 Data Definition Directives]()
 * [17.3.6 EQU Directive]()
 * [17.4 80x86 Opcodes]()
 * [17.4.1 Data Transfer]()
 * [17.4.2 Data Manipulation]()
 * [17.4.3 Program Control]()
 * [17.4.4 Special Operations]()
 * [17.5 Addressing Modes]()
 * [17.5.1 Register Addressing]()
 * [17.5.2 Immediate Addressing]()
 * [17.5.3 Pointer Addressing]()
 * [17.6 Sample 80x86 Assembly Language Programs]()
 * [17.7 Additional 80x86 Programming Resources]()
 * [17.8 What's Next?]()
 * [Problems]()


# Computer Organization and Design Fundamentals

# 1. [Chapter One: Digital Signals and Systems](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#1-chapter-one-digital-signals-and-systems-1)

Knowing how to design and build a computer may not be vital to the computer professional, but it goes a long way toward improving their skills, but example, making them better drivers.

The principles of computer organization provide tools to create better designs.

	* System Design Tools.
	* Software Design Tools.
	* Improved troubleshooting skills.
	* Interconnectivity
	* Marketability

> When you control parts design, you can integrate the whole package much more elegantly.

# * [1.1 Should Software Engineers Worry About Hardware?](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#1-chapter-one-digital-signals-and-systems-1)


![Screen Shot 2020-06-05 at 15 23 34](https://user-images.githubusercontent.com/24994818/83919628-9f43fd00-a740-11ea-910f-f5257759b79e.png)

# * [1.2 Non-Digital Signal](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#1-chapter-one-digital-signals-and-systems-1)

The real world is analog. When values such as temperature or weight change over time, they follow what is called a **continuous curve**. It is sufficient to say that analog values represent a continuous signal with infinitesimal resolution.

# * [1.3 Digital Signals](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#1-chapter-one-digital-signals-and-systems-1)

There is such a thing as an analog computer, a computer that processes information using analog levels of electricity or the positions of mechanical devices. Now computers represent an analog value by converting it to a number with a fixed resolution. This measurement is referred to as a **digital value**.

![Screen Shot 2020-06-06 at 13 03 35](https://user-images.githubusercontent.com/24994818/83951299-269f7800-a7f6-11ea-858b-63451d4794b2.png)

The computer can only measure the signal at intervals. Each measurement is called a **sample**. The rate at which these samples are taken is called the **Sampling rate**.

![Screen Shot 2020-06-06 at 13 05 13](https://user-images.githubusercontent.com/24994818/83951333-61091500-a7f6-11ea-9c89-f0d55adb0702.png)

Two problem arise from this process: information can be lost between the measurement and information can be lost due to the **rounding** of the measurement. First, if the sampling rate is too slow, the some details of the signal may be missed.

![Screen Shot 2020-06-06 at 13 07 18](https://user-images.githubusercontent.com/24994818/83951375-ac232800-a7f6-11ea-8b82-7b3a0126502b.png)

Second, if the computer does not record with enough accuracy an error may be introduced between the actual measurement and the recorded value.

![Screen Shot 2020-06-06 at 13 08 48](https://user-images.githubusercontent.com/24994818/83951394-e42a6b00-a7f6-11ea-87a2-fff593ffe989.png)

These effects can be reduced by increasing the resolution of the measurement and increasing the sampling rate. A discussion of this can be found in Chapter 2 in the section titled "Sampling Theory"	

# * [1.4 Conversion Systems](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#1-chapter-one-digital-signals-and-systems-1)

The typical system used to convert an external condition such as pressure, temperature, or light intensity to a format usable by a digital system is shown in the block diagram.

![Screen Shot 2020-06-07 at 12 00 01](https://user-images.githubusercontent.com/24994818/83974936-72195b00-a8b6-11ea-8ac5-3703e364fea6.png)

The interface between the external condition and the electronics of the system is the sensor. This device converts the environmental conditions into a signal readable by analog electronics. Often, this signal is weak and is easily distorted by noise. Therefore, the output of the sensor is usually amplified and cleaned up before being converted to digital values by the Analog-to-Digital Converter (ADC).

Continuous operation of this system results in a sequence of digital measurements or samples that are stored in the computer where it can be viewed much like the table of numbers in a spreadsheet.

First benefit of digital systems: If an analog signal is transmitted over long distances, noise attaches itself to the signal. To keep the signal strong enough to reach its destination,it must be amplified. All of the noise attached itself to the signal, however, is amplified along with the original signal resulting in distortion. 

Noise cannot attach itself to a digital signal. Once an analog signal has been converted to a sequence of numbers, the signal's characteristics remain the same as long as the number don't change. Therefore, digital systems such as contemporary long-distance phone system do not suffer from degradation over long distances.

A second benefit is that once a signal is turned into a sequence of numbs, mathematical algorithms can be used to operate on the data. Disciplines such as Digital Signal Processing and the study of wavelets allow for much more accurate processing of signals that analog systems were ever able to achieve.

These advantages come at a price, however. As mentioned earlier, if the samples are taken too slowly, details of the analog input are missed. It the resolution of the samples is not fine enough, the signal may not be precisely represented with the signal values. Last of all, additional hardware is required to convert the signal from analog to digital

# * [1.5 Representation of Digital Signals](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#1-chapter-one-digital-signals-and-systems-1)

Digital Systems do not store numbers the way humans do. Digital systems work with numbs using millions of tiny switches called transistors. Each transistor can remember only one f two possible values, on or off. This referred to as a **binary system**.

The complexity of the computer comes in how the millions of transistors are designed to work together. The purpose of this discussion, the two values of a transistor will be referred to as **logic 1** and **logic 0**.

![Screen Shot 2020-06-08 at 18 01 42](https://user-images.githubusercontent.com/24994818/84088447-21872800-a9b2-11ea-9a44-da484e46bd53.png)

Sometimes, two or more binary lines are grouped together to perform a single function. 

![Screen Shot 2020-06-08 at 18 02 44](https://user-images.githubusercontent.com/24994818/84088508-44b1d780-a9b2-11ea-9afc-8a9e032bfa1a.png)

Alternatively, multiple lines can be combined into a more abstract representation such as the one shown in Figure 1-10

![Screen Shot 2020-06-08 at 18 04 00](https://user-images.githubusercontent.com/24994818/84088582-71fe8580-a9b2-11ea-900c-0d08793f68a6.png)

Hash marks indicate invalid or changing data. This could mean that one or all of the signals are changing their values, or that due to the nature of the electronics, the values of the data signals cannot be predicted. In the later case, the system may need to wait to allow the signals to stabilize. 

# * [1.6 Types of Digital Signals](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#1-chapter-one-digital-signals-and-systems-1)
# * [1.6.1 Edges](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#1-chapter-one-digital-signals-and-systems-1)

A single binary signal can have one of two possible transitions as shown in figure 1-11. The first one, a transition from a logic 0 to a logic 1, is called a rising edge transition. The second one, a transition from a logic 1 to a logic 0 is called a falling edge transition.

![Screen Shot 2020-06-08 at 18 12 18](https://user-images.githubusercontent.com/24994818/84089054-9dce3b00-a9b3-11ea-82e0-76c251d12674.png)

# * [1.6.2 Pulses]()

A binary pulse occurs when a signal changes from one value to the other for a short period, then returns to its originals value. Examples of this type of signal might be the power-on or reset buttons on a computer (momentarily pressed, the released) or the button used to initialize synchronization between a PDA and a computer.

![Screen Shot 2020-06-08 at 18 13 08](https://user-images.githubusercontent.com/24994818/84089084-b9d1dc80-a9b3-11ea-8647-b3e49af21a70.png)

There are two types of pulses. The first is called a **positive-going-pulse**, and it has an idle state of logic 0 with a short pulse to logic 1. The other one, a **negative-going pulse**, has an idle state of logic 1 with a short pulse to logic 0. Both of these signals are shown in Figure-1-12.

# * [1.6.3 Non-Periodic Pulse Trains](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#1-chapter-one-digital-signals-and-systems-1)

Some digital signals such as the data wires of an Ethernet link or the data and address lines of a memory interface do not have a characteristic pattern in their changes between logic 1 and logic 0. These are called **non-periodic pulse trains**

![Screen Shot 2020-06-08 at 18 18 44](https://user-images.githubusercontent.com/24994818/84089414-86438200-a9b4-11ea-8c43-9d8d81090287.png)

Like music, the duration of the notes or the spaces between the notes can be longer or shorter. On the page, they do not look meaningful, but once the reader is given the tools to interpret the signal, he data they contain becomes clear.

# * [1.6.4 Periodic Pulse Trains](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#1-chapter-one-digital-signals-and-systems-1)

Some signals act as the heartbeat to a digital systems. Periodic pulse trains is meat to synchronized events or keep processes moving.

![Screen Shot 2020-06-09 at 13 36 54](https://user-images.githubusercontent.com/24994818/84186574-5947ab80-aa56-11ea-8d75-1381c6b0c827.png)

![Screen Shot 2020-06-09 at 13 37 49](https://user-images.githubusercontent.com/24994818/84186627-6d8ba880-aa56-11ea-9b80-2eb9cbb9f4a0.png)

Frequency = 1 / Period [seconds]

# * [1.6.5 Pulse-Width Modulation](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#1-chapter-one-digital-signals-and-systems-1)

The last measurement of a periodic waveform is the **duty cycle**. The duty cycle represents the percentage of time that a periodic signal is a logic 1. 

![Screen Shot 2020-06-09 at 13 40 36](https://user-images.githubusercontent.com/24994818/84186884-d07d3f80-aa56-11ea-8eda-203e280a6815.png)

Duty Cycle = ((logic 1 pulse duration) [seconds] / period [Seconds] ) * 100%

# * [1.7 Unit Prefixes](https://github.com/c4arl0s/ComputerOrganization-DesignFundamentals#1-chapter-one-digital-signals-and-systems-1)

![Screen Shot 2020-06-09 at 13 44 01](https://user-images.githubusercontent.com/24994818/84187367-8f395f80-aa57-11ea-9004-5a21b5cbac9b.png)

# * [1.8 What's Next?]()

In this chapter, we have seen how the methods that a computer uses to store and interpret values are different from the ways in which those values appear in the real world. We have also seen some of the methods used to measure and represent these digital signals.

In Chapter 2 we will see how digital values are used to represent integers. This is the first major step toward understanding some of the idiosyncrasies of computing systems such as why a compiler might restrict the values of a data type from -32,768 to 32,767. In addition, it shows how some bugs occur in program due to the misuse of data types.

# * [Problems]()

# 2. [Chapter Two: Numbering Systems]()

Chapter one discussed how computers remember numbers using transistors, tiny devices that act like switches with only two positions, on or off. A single transistor, therefore, can only remember one of two possible numbers, a one or a zero. This is not useful for anything more complex than controlling a light bulb, so for larger values, transistors are grouped together so that their combination of ones and zero can be used to represent larger numbers.

This chapter discusses some of the methods that are used to represent numbers with groups of transistors or **bits**. The reader will also be given methods for calculating the minimum and maximum values of each representation based on the number of bits in the group.

# * [2.1 Unsigned Binary Counting]()

The simplest form o numeric representation with bits is **unsigned binary**. When we count wpward through the positive integers using decimal, we start with a 0 in the one's place and increment a value until we reach the upper limit of a single digit, i.e. 9. At that point, we have run out of the "Symbols" we use to count, and we need to increment the next digit, the ten's place. We then reset the one's place to zero, and start the cycle again.

![Screen Shot 2020-06-10 at 19 12 19](https://user-images.githubusercontent.com/24994818/84330922-54feb980-ab4e-11ea-9296-cd06592b85da.png)

Figure 2-2 shows that when counting in binary, we run out of symbols quickly requiring the addition of another "place" only the second increment.

![Screen Shot 2020-06-10 at 19 13 59](https://user-images.githubusercontent.com/24994818/84330983-8e372980-ab4e-11ea-9426-aefc8ba399ea.png)

With 2 symbols for each bit, we have 2^n possible combinations of symbols, where n represents the number of bits.	

Figure 2-3 uses 5 bits to count up to decimal 17. Examine each row where a single one position is represent in the binary number. This reveals what that position represents. For example, a binary 01000 is shown to be equivalent to a decimal 8. Therefore, the fourth bit position from the right is the 8`s position.

![Screen Shot 2020-06-10 at 19 15 38](https://user-images.githubusercontent.com/24994818/84331052-c9395d00-ab4e-11ea-9d49-049aa53c66a4.png)

This information will help us develop a method for converting **unsigned binary numbers** to decimal and back to **unsigned binary**.

Some of you may recognize this as "base-2" math. This give us a method for indicating whic representation is being ued when writing a number down on paper. 

# * [2.2 Binary Terminology]()

When writing values in decimal, it is common to separate the places or positions of large numbers in groups of three digits separated by commas.

To begin with, a single place or position in a binary number is called a bit, short for binary digit. 

Th rightmost bit, the one that represents the ones places, is called the **Least Significant Bit or LSB***.

The leftmost bit, the one that represents the ones places, is called the **Most Significant Bit or MSB, 

 * Nibble - A four bit binary number. (4 bit)
 * Byte - A unit of storage of a single character, typically an eight bit (2 nibble) binary number (short for binary term)
 * Word - Typically a sixteen bit (16 bit) (2 byte) binary number.
 * Double Word - A thirty-two bit (32 bit) (2 word) binary number.

The following are some examples of each type of binary number.

Bit								1
Nibble  					10102
Byte							101001012
Word 							1010010111110000
Double Word				10100101111100001100111011101101

# * [2.3 Unsigned Binary to Decimal Conversion]()

![Screen Shot 2020-06-11 at 23 38 07](https://user-images.githubusercontent.com/24994818/84465458-a6857200-ac3c-11ea-9820-5618d0954a20.png)

It turns out that can be represented with n bits in unsigned binary is:

2^n - 1    (remember to include zero)


# * [2.4 Decimal to Unsigned Binary Conversion]()

Converting from decimal to unsigned binary is a little more complicated, but it still is not too difficult. Once again, there is a well-defined process.

![Screen Shot 2020-06-11 at 23 45 53](https://user-images.githubusercontent.com/24994818/84465879-b9e50d00-ac3d-11ea-8e5f-f82d3caac07f.png)

# * [2.5 Binary Representation of Analog Values]()

Converting unsigned (positive) integers to binary is only one of the many ways that computers represent values using binary bits. This chapter still has two more to cover, and Chapter 3 will cover even more.

This section focuses on the problems and solutions of trying to map real world values such as temperature or weight from a specified range to a binary integer. 

For example a computer that uses 8 bits to represent an integer is capable of representing 256 individual values from 0 to 255. Temperature, however, is a floating point value with unrealistic upper and lower limits. Can we get a computer to represent a temperature using eight bits ? The answer is yes, but it will cot us in the areas of resolution and range.

![Screen Shot 2020-06-12 at 19 48 08](https://user-images.githubusercontent.com/24994818/84556023-b5742f00-ace5-11ea-9863-c93f4c1d8fb5.png)

For example, does the typical bathroom scale need to measure values above 400 pounds? If not, then a digital system could use a 10-bit binary number mapped to a range fro zero to 400 pounds. A binary 00000000000 could represent zero pounds to 1111111111 could represent 400 pounds

What is needed next is a method to map the values inside the range zero to 400 pounds to the binary integers in the range of 0000000000 to 1111111111. To do this, we need a linear function defining a one-to-one mapping between each binary integer and the analog value it represents. To do this, we turn to the basic math expression for a linear function.
 
This function defines m as the rate of the change in y with respect to changes in x and b as the value y is set to when x equals 0. We can use this expression to map a binary integer x to an analog value y.

The slope of the function, m, can be calculated by dividing the range of analog values by the number of intervals defined by the n-bit binary integer. The number of intervals defined by the n-ñbit binary integer is equal to the upper limit of that binary number if it were being used as an unsigned integer 2^n ´- 1.

![Screen Shot 2020-06-12 at 19 50 08](https://user-images.githubusercontent.com/24994818/84556056-f2402600-ace5-11ea-9c82-a1b39a1c99f4.png)

![Screen Shot 2020-06-12 at 19 50 39](https://user-images.githubusercontent.com/24994818/84556063-0126d880-ace6-11ea-8a53-544ca4fa8169.png)

Lets go back to our example of the kitchen scale where the maximum analog value is 400 pounds while the minimum is zero pounds. If a 10-bit binary value is used to represent this analog value, then the number of intervals of the binary integer is 2^10-1 = 1023

m = (400 pounds - 0 pounds) / 1023 binary increments = 0.391 pounds / binary increment

That means each time the binary number increments 0110110010 goes to 0110110011 , it represents an increment in the analog value of 0.391 pounds. 

In this case you use 10 bits, now see the example whe the increment per each binary increent is required. [See example]

In some cases, the lower limit might be something other than 0. This is important especially if better accuracy is required

b = minimum analog value

![Screen Shot 2020-06-15 at 9 22 37](https://user-images.githubusercontent.com/24994818/84668938-d7d69a00-aee9-11ea-800d-c3c09de734ff.png)

# * [2.6 Sampling Theory]()

In general, an n-bit analog-to-digital converter divides the analog range into 2^n-1 increments.

![Screen Shot 2020-06-15 at 9 25 46](https://user-images.githubusercontent.com/24994818/84669200-369c1380-aeea-11ea-8695-507217c8bcba.png)

The figure shows how the addition of a bit can improve the resolution of the values represented by the binary integers.

To improve the signal's representation, the rate at which the samples are taken, the **sampling rate**, needs to be increased. 

There is also a chance of missing a higher frequency because the sampling rate is too slow. This is called **aliasing**, and there are examples of it in everyday life.

If a signal's frequency is faster than the sampling rate, then information will be lost, and the collected data will never be able to duplicate the original

![Screen Shot 2020-06-15 at 9 31 50](https://user-images.githubusercontent.com/24994818/84669929-0dc84e00-aeeb-11ea-89d8-5be9a7ea8896.png)

To avoid **aliasing**, the rate at which samples are taken must be more than twice as fast as the highest frequency you wish to capture. This is called the **Nuquist Theorem**.

# * [2.7 Hexadecimal Representation]()

To make the binary representation of numbers easier on us humans, there is a shorthand representation for binary numbers. It begins by partitioning a binary numbers into its nibbles starting at the least significant bit (LSB). 

![Screen Shot 2020-06-15 at 9 45 24](https://user-images.githubusercontent.com/24994818/84671402-f5593300-aeec-11ea-8571-03d6d2b28168.png)

![Screen Shot 2020-06-15 at 9 46 26](https://user-images.githubusercontent.com/24994818/84671517-19b50f80-aeed-11ea-89d9-3b224a04ae99.png)

![Screen Shot 2020-06-15 at 9 47 03](https://user-images.githubusercontent.com/24994818/84671586-2e91a300-aeed-11ea-86e2-67d0e5aa5285.png)


Hexadecimal provides humans with a reliable, short-hand method of writing large binary numbers.

# * [2.8 Binary Coded Decimal]()

Binary Coded Decimal allows for fast conversion to binary of integers that do not require mathematical operations.

As in hex, each decimal digit represent a nibble of the binary equivalent Table 2-2 shows the conversion between each decimal digit and the binary equivalent.

![Screen Shot 2020-06-16 at 17 48 57](https://user-images.githubusercontent.com/24994818/84835656-ae0c9880-aff9-11ea-982c-1ada5b81c4f2.png)

It is important to note that there is no algorithmic conversion between BCD and decimal. BCD is only a method for representing decimal numbers in binary.

BCD is used frequently in financial applications due to legal requirements that decimal values be **exactly** represented. 

 * It is more complex to do mathematical operations
 * May require more memory for storage
 * Implementations could use this binary code to use signed numbers.


# * [2.9 Gray Codes]()

The use of binary counting sequences is common in digital applications. For example an n-bit binary value can be used to identify the position of a rotating shaft as being within one of 2^n different arcs.

As the shaft turns, a sensor can detect which of the shaft's arcs it is aligned with by reading a digital value and associating it with a specific arc. By remembering the previous position and timing the changes between positions, a processor can also compute speed and direction.

![Screen Shot 2020-06-17 at 13 31 37](https://user-images.githubusercontent.com/24994818/84935830-ea460480-b09e-11ea-9a19-623320f4d0d7.png)

![Screen Shot 2020-06-17 at 13 32 15](https://user-images.githubusercontent.com/24994818/84935860-f92cb700-b09e-11ea-8b2a-b0c431e65af0.png)

There is a potential problem with this methods o encoding. It is possible to read the sensor at instant when more than one gap is opening or closing between its light source and sensor. When this happens, some of the bit changes may be detected while others are not. If this happens, an erroneous measurement may occur.

For example, if the shaft shown above turns clockwise toward position 101 = 5, but at the instant when the value read will be 111 = 7, indicating counter-clockwise rotation.

To solve this problem, alternate counting sequences referred to as **Gray Code** are used. These sequences have only one bit change between values. For example, the values assigned to the arcs of the above shaft could follow the sequence 000, 001, 011, 010, 110, 111, 101, 100. **This sequence is not correct numerically, but as the shaft turns, only one bit will change as the shaft turns from one position to the next**.

![Screen Shot 2020-06-17 at 13 42 51](https://user-images.githubusercontent.com/24994818/84936827-74db3380-b0a0-11ea-9853-ac464de19042.png)

![Screen Shot 2020-06-17 at 13 43 20](https://user-images.githubusercontent.com/24994818/84936867-86bcd680-b0a0-11ea-9391-10640aa31ec9.png)

Notice that exactly one bit changes in the Gray code from one row to the next and from the bottom row to the top row.

# * [2.10 What's Next?]()

In this chapter, we have covered the difference methods of representing values, specifically positive integers, using digital circuitry. In addition to counting integers, the issues surrounding the conversion of analog or "real world" values to digital were examined along with some of the problems encountered when sampling. Finally, two methods of binary representation were presented: **Hexadecimal** and **BCD**.

Chapter 3 examines the special needs surrounding the digital representation of addition, subtraction, and floating-point values. It introduces the operation of the processor in handling some arithmetic functions.

# * [Problems]()

# 3. [Chapter Three: Binary Math and Signed Representations]()

Representing numbers with bits is one thing. Doing something with them is an entirely different matter. This chapter discusses some of the basic mathematical operations that computers perform on binary numbers along with the binary representations that support those operations. These concepts will help programmers better understand the limitations of doing math with a processor, and thereby allow them to better handle problems such as the upper and lower limits of variable types, mathematical overflow, and type casting.

# * [3.1 Binary Addition]()

![Screen Shot 2020-06-17 at 17 56 10](https://user-images.githubusercontent.com/24994818/84958882-e678a900-b0c3-11ea-81d4-520065321b73.png)

![Screen Shot 2020-06-17 at 17 57 56](https://user-images.githubusercontent.com/24994818/84958979-16c04780-b0c4-11ea-945e-bf9d220eb46b.png)

![Screen Shot 2020-06-17 at 17 58 17](https://user-images.githubusercontent.com/24994818/84959006-2475cd00-b0c4-11ea-86f2-0081eb264abb.png)

![Screen Shot 2020-06-17 at 17 59 10](https://user-images.githubusercontent.com/24994818/84959084-496a4000-b0c4-11ea-9d0e-15964b586d95.png)

# * [3.2 Binary Subtraction]()

![Screen Shot 2020-06-18 at 12 56 54](https://user-images.githubusercontent.com/24994818/85055474-39ee0400-b163-11ea-9802-e48cc1fe9457.png)

![Screen Shot 2020-06-18 at 12 58 23](https://user-images.githubusercontent.com/24994818/85055592-686bdf00-b163-11ea-9e01-70635de0d2c3.png)

2^0 column: Starting at the rightmost bit, is subtracted from 1 giving us zero.
2^1 column: 0 is subtracted from 1 resulting in 1
2^2 column: 1 is subtracted from 0. Here we need to borrow from the next highest bit. The next highest digit is 1, so we subtract 1 from it and add 10 to this digit. It is shown a small 1 before the 0. This makes our subtraction 10-1 which is equals to 1. It means 2-1 = 1
2^3 column: After the borrow, we have 0-0 which equals 0. 
2^4 column: 1 - 1 = 0
2^5 column: 1 - 0 = 0
2^6 column: we find 0 - 1 again. We need to make a borrow again in the third column from the left, but the 2^7 position of the minuend is zero and does not have anything to borrow. Therefore, the next highest digit of the minuend, the 2^8 position, is borrowed from. The borrow is then cascaded down until it reaches the 2^6 position so that the subtraction may be performed.

![Screen Shot 2020-06-18 at 13 48 29](https://user-images.githubusercontent.com/24994818/85060181-69ecd580-b16a-11ea-9dfe-f448cd2650d4.png)

# * [3.3 Binary Complements]()

In decimal arithmetic, every number has an additive complement i.e., a value that when added to the original number results in a zero. For example, 5 and -5 are additive complements because 5+(-5)=0. This section describes the two primary methods used to calculate the complements of a binary value.

# * [3.3.1 One's Complement]()

Flip each bit in the original value

![Screen Shot 2020-06-19 at 18 40 28](https://user-images.githubusercontent.com/24994818/85186048-66397b80-b25c-11ea-8715-bfed77889d89.png)

The 1's complement of a value is useful for some types of digital functions, but it does not provide much of a benefit if you are looking for the additive complement. See whats happen when we add a value to its 1's complement

![Screen Shot 2020-06-19 at 18 43 38](https://user-images.githubusercontent.com/24994818/85186121-cd573000-b25c-11ea-855d-14a73293f148.png)

If the two values were additive complements, the result should be zero, right ? Well, that takes us to the 2's complement.


# * [3.3.2 Two's Complement]()


The result of adding an n-bit number to its one's complement is always an n-bit number with ones in every position. If we add 1 to that result, our new value is an n-bit number with zeros in every position and an overflow or carry to the next highest position, The (n+1)^th column which corresponding to 2^n. 

For our 8-bit example above, the result of adding 10010110 to 01101001 is 1111111. Adding 1 to this number gives us 00000000 with and overflow carry of 1 to the ninth or 2^8 column. If we restrict ourselves to 8 bits, this overflow carry canbe ignore.

So, the 2's complement of a value is found by firts taking the 1's complement, then incrementing that result by 1. 

![Screen Shot 2020-06-19 at 18 43 38](https://user-images.githubusercontent.com/24994818/85186316-eca28d00-b25d-11ea-926c-03141a2dfa11.png)

Then we can test

![Screen Shot 2020-06-19 at 18 52 11](https://user-images.githubusercontent.com/24994818/85186334-03e17a80-b25e-11ea-9a34-08d577232cfe.png)

Another example 88 - 10 = 78

88 	-	01011000
-10 -	11110110

![Screen Shot 2020-06-19 at 18 56 29](https://user-images.githubusercontent.com/24994818/85186436-98e47380-b25e-11ea-97cf-31895ccf1e62.png)

result = 2^6+2^3+2^2+2^1 = 78

![Screen Shot 2020-06-19 at 18 59 11](https://user-images.githubusercontent.com/24994818/85186497-f973b080-b25e-11ea-9480-32741f85b6be.png)

This match the result for the previous example.

Also remember, is we want to obtain the negative number of (-5) is five, so taking the two's complement you get 5.

![Screen Shot 2020-06-19 at 19 01 27](https://user-images.githubusercontent.com/24994818/85186541-4b1c3b00-b25f-11ea-84fd-0bb64b6e007a.png)

# * [3.3.3 Most Significant Bit as a Sign Indicator]()

MSB of a value can be used to indicate whether a number is positive or negative and is called a **sign bit**.

> A binary value with a 0 in the MSB position is considered positive and a binary value with a 1 in the MSB position is considered negative.

Since the MSB is being used to indicate the sign of a signed binary number, it cannot be used to represent a power of 2. If a number is said to represent a 2's complement value, only n-1 of its n bits can be used to determine the magnitude since the MSB is used for the sign.

This cuts in half the number of positive integers n bits can represent.

Special Cases ?

- Binary number with all zeros is equal to a decimal 0.
- Taking the negative of zero still gives us zero.
- In this section on minimums and maximums, we will see that an n-bit value with an MSB equal to one and all other bits equal to zero is a negative number, specifically, -2^(n-1). 
- The larges positive number represented in 2's complement has an MSB of 0 with all the remaining bits set to one. This value equals [2^(n-1) - 1]. Therefore, since 2^(n-1) is greater than [2^(n-1) -1], we cansee that there s no positive equivalent to the binary number 1000...000.

# * [3.3.4 Signed Magnitude]()

A second, less useful way to represent positive and negative binary numbers is to take the MSB and use it as a sign but, much like a plus or minus sign, and leave the remaining bits to represent the magnitude.

![Screen Shot 2020-06-21 at 18 17 00](https://user-images.githubusercontent.com/24994818/85237300-6caa2d80-b3eb-11ea-873e-66c2c4fe03b7.png)

# * [3.3.5 MSB and Number of Bits]()

Since the MSB is necessary to indicate the sign of a binary value, it is vital that we know how many bits a particular number is being represented with so we know exactly where the MSB is. In other words, the leading zeros of a binary value may have been removed making it look like the binary value is negative since it starts with a one.

# * [3.3.6 Issues Surrounding the Conversion of Binary Numbers. 52 ]()

Since computers don't use an infinite number of bits to represent values, the software must know two things before it can interpret a binary value: the number of bits and the type of binary representation being used. This usually is confusing for the novice.

![Screen Shot 2020-06-25 at 15 59 32](https://user-images.githubusercontent.com/24994818/85794897-fb28f280-b6fc-11ea-95b2-9095f53cd13d.png)

{insert explanation about another conversion}

This discussion shows that it is possible for a binary pattern of ones and zeros to have three interpretations. It All depends on how the computer has been told to interpret the value

In a programming language such as C, the way in which a computer treats a variable depends on how it is declared. Variables declared as **unsigned int** are stored in **unsigned binary notation**. Variables declared as **int** are treated as either **2's complement** or **signed magnitud depending on the processor and/or compiler.

# * [3.3.7 Minimums and Maximums]()

When using a finite number of bit positions to store information, it is vital to be able to determine the minimum and maximum values that each binary can handle. Failure to do this might result in bugs in the software you create. This section calculates the minimum and maximum values for each of the three representations discussed in this and the previous chapter using a fixed number of bits, n.

Let's begin with the most basic representation, **unsigned binary**. The smallest valye that cn be represented with **unsigned binary representation** ocurrs when all the bits equal to zero. Conversion from binary to decimal results 0+0+...+0=0, Therefore, for an n bit number

Minimum n-bit unsigned binary number = 0

The largest value that can be represented with unsigned binary representation is reached when all n bits equal to one. When we convert this value from binary to decimal, we get 2^(n-1) + 2^(n-2)+...+2^0. As was shown in Chapter 2, adding one to this expression results in 2^n.

Therefore, for an n-bit unsigned binary numberm the maximum is:

Maximum n-bit unsigned binary number = 2^n - 1

And so on.

![Screen Shot 2020-06-25 at 16 18 05](https://user-images.githubusercontent.com/24994818/85796400-83a89280-b6ff-11ea-8a0f-21f5cd6ea206.png)

So **why can 8-bit signed magnitude only represents 255 possible values instead of 256? It is because in signed magnitude 00000000 and 1000000 both represent the same number, a decimal 0.


# * [3.4 Floating Point Binary]()

Binary numbers can also ave decimal points, and to show you how, we will once again begin with decimal numbers. For decimal numbers with decimal points, the standard way to represent the digits to the right o the decimal points is to continue the powers of ten in descending order starting with -1 where 10^(-1) = 1/10 = 0.1. That means that the number 6.5342 has 5 increments of 10^(-1) (tenths), 3 increments of 10^(-2) (hundredths), 4 increments of 10^(-3) (thousandths), and 2 increments of 10^(-4) (tn-thousandths) The table below shows this graphically.

| Exponent       | 3    | 2   | 1  | 0 | -1  | -2   | -3    | -4      |
|----------------|------|-----|----|---|-----|------|-------|---------|
| Position Value | 1000 | 100 | 10 | 1 | 0.1 | 0.01 | 0.000 | 0.00001 |
| Sample Values  | 0    | 0   | 0  | 6 | 5   | 3    | 4     | 2       |

Binary representation of real numbers works the same way except that each position represents a power of two, not a power of ten

| Exponent       | 2 | 1 | 0 | -1  | -2   | -3    | -4    | -5      |
|----------------|---|---|---|-----|------|-------|-------|---------|
| Position Value | 4 | 2 | 1 | 0.5 | 0.25 | 0.125 | 0.065 | 0.03125 |
| Sample Values  | 0 | 1 | 0 | 0   | 1    | 1     | 0     | 1       |

Computers, use a form of binary more like scientific notation to represent floating-point or real numbers.

The IEEE standard 754 is used to represent real numbers on the majority of contemporary computer systems. It utilizes a 32-bit pattern to represent single-precision numbers and a 64-bit pattern to represent double-precision numbers. Each of these bit patterns is divided into three parts, each part representing a different component of the real number being stored.


![Screen Shot 2020-06-28 at 6 30 17](https://user-images.githubusercontent.com/24994818/85946267-e91b9f80-b908-11ea-9ded-c45cba8494f0.png)

Both formats work the same differing only by the number of bits used to represent each component of the real number. In general, the components of the single-precision format are substited into Equation 3.7 where the sign of the value is determined by the sign bit (0 - positive value, 1 - negative value). Note that E is in unsigned binary representation.

(+/-)1.Fx2^(E-127)


for double-precision values

(+/-)1.Fx2^(E-127)

In both cases, F is preceded with an implied '1' and a binary point.
There are, however, some special cases. These are as follows:

- Positive, E=255, F=0: represents positive infinite;
- Negative, E=255, F=0: represents negative infinite; and 
- Positive or negative, E=0, F=0; represents zero.


# * [3.5 Hexadecimal Addition]()

In decimal does not require a carry until the result goes beyond 9. Hexadecimal numbers (base 16) can be added using the same method. 

![Screen Shot 2020-06-28 at 20 36 57](https://user-images.githubusercontent.com/24994818/85964402-2b7fc380-b97f-11ea-81f9-9b86fc7f8f8c.png)

For example, in decimal, adding 5 and 7 results in 2 with a carry to the next highest position. In hexadecimal, however, 5 added to 7 dos not go beyond the range of a single digit. In this case, 5+7=(C)16 with no carry. It is not until a result greater than (F)16 is reached (a decimal (15)10) that a carry is necessary.

Example

Add (3DA32)16 to (4292F)16

|   | 1 | 1 |   | 1 |   |
|---|---|---|---|---|---|
|   | 3 | D | A | 3 | 2 |
| + | 4 | 2 | 9 | 2 | F |
|   | 8 | 0 | 3 | 6 | 1 |


# * [3.6 BCD Addition]()

When we introduced Binary Coded Decimal numbers, we said that the purpose of these numbers was to provide a quick conversion to binary that would not be used in mathematical functions. It turns out, however, that BCD numbers can be added too, there is just an additional step that occurs when each column of digits is added.

When two BCD numbers are added, the digits 1010, 1011, 1100, 1101, 1110 and 1111 must be avoided. This is done by adding an additional step anytime the binary addition of two nibbles results in one of these illegal values or if a carry is generated. When this happens, the invalid result is corrected by adding 6 to skip over the illegal values:

For example:

![Screen Shot 2020-06-29 at 19 02 54](https://user-images.githubusercontent.com/24994818/86067819-592a4280-ba3b-11ea-9f5e-3781a4914f63.png)

This step is also necessary if a carry results from a BCD addition.

![Screen Shot 2020-06-29 at 19 03 02](https://user-images.githubusercontent.com/24994818/86067822-5a5b6f80-ba3b-11ea-8dad-47c36531e207.png)

# * [3.7 Multiplication and Division by Powers of Two]()

Due to factors to be examined later on this book, multiplication and division is a time-intensive operation for processors. Therefore, programmers and compilers have a trick they use to divide or multiply binary by powers of two. Examine Table 3-3 to see if you can find a pattern in the multiples of two of the binary number (1001)2.

![Screen Shot 2020-06-30 at 18 25 25](https://user-images.githubusercontent.com/24994818/86186641-35303500-baff-11ea-830a-2dba1c6de54d.png)

Note that multiplying by two has the same effect as shifting all of the bits one position to the left. Similarly, a division by two is accomplished by **a right shift one position**. This is similar to moving a decimal point right or left when multiplying or dividing a decimal number by a power of ten.

Since a shift operation is significantly faster than a multiply or divide operation, compilers will always substitute a shift operation when a program calls for a multiply or divide by a power of two. For example, a division by 16 = 2^4 is equivalent to a right shift by 4 bit positions.

This works for all positive binary representations of integers and real numbers as well as 2's complement representation of negative numbers. Care must be taken in a few instances in order to maintain the data's integrity.

First, carefully watch the bits that are shifted out to verify that data ins not being lost. If during a left shift (multiplication), a one is shifted out of an unsigned binary value or the MSB of 2's complement number changes, then you have gone beyond the range of values for that number of bits. If during a right shift (division), a one is shifted out of an integer value, then a decimal value has been truncated.

For negative 2's complement values, there is an additional concern. Since the MSB is a sign bit,if we fill in the empty bits coming in from the left with zeros when performing  right shift, then a negative number has been turned into a positive number. To avoid this, always duplicate the sign bit in the MSB for each right shift of 2's complement value

![Screen Shot 2020-06-30 at 18 38 32](https://user-images.githubusercontent.com/24994818/86187228-ec797b80-bb00-11ea-9b5c-3cef7b15d942.png)

This operation can be used for some multiplications by constants other than powers of two. For example, if a processor needed to multiply a value x by 10, it could first multiply x by 2 (a single left shift), then multiply x by 8 ( a left shift by three bit positions=, then add the two shifted values together. This would still be a time savings over a multiplication.

x * 10 = x * (2 + 8) = x *2 + x * 8

A bit shift is easily accomplish in hight-level programming languages such as C. In C, the operator used to perform a left shift is '<<' while  a right shift is '>>'. Place the variable to be shifted to the left of the operator and to the right of the operator, enter the number of positions to shift. Some sample C codeis shown bellow.

```c
result = iVal << 3 // set result equal to iVal shiften left 3 places
result = iVal >> 4 // set result equal to iVal shiften right 4 places.
```

The first line of code shifts iVal left three positions before putting the new value into **result**. This equivalent to multiplying iVal by 2^3 = 8. The second line shifts iVal right 4 positions which has the same effect as an integer divide by 2^4 = 16.

# * [3.8 Easy Decimal to Binary Conversion Trick]()

The fact tha a single shift right is equivalent to a division by two give us a simple way to convert from decimal integers to unsigned binary. Each 1 that is shifted out because of a right shift is equivalent to a remainder of 1 after a division by two. Therefore, if you record the remainders generated by successive divisions by two, you will find that you have generated the binary equivalent of the original decimal value. For example, let's convert the decimal value 156 to binary.

156 % 2 = 78 with a remainder of 0
78 % 2 = 39 with a remainder of 0
39 % 2 = 19 with a remainder of 1
19 % 2 = 9 with a remainder of 1
9 % 2 = 4 with a remainder of 1
4 % 2 = 2 with a remainder of 0
2 % 2 = 1 with a remainder of 0
1 % 2 = 0 with a remainder of 1

Listing the remainders by reversing the order in which they were generatde gives us 10011100, the binary value of 156.

Woow !

# * [3.9 Arithmetic Overflow]()

In Section 3.3, the carry was ignored when two 2's complement values were added. This is not always the case. **For some numbering systemsl, a carry is an indication that an error has occured**.

An arithmetic overflow error occurs when two numbers are added and the result falls outside the valid range of the binary representation being used. For example the numbers 200 and 175 can be representated in 8-bit unsigned binary notation. The result of their addition, however, 375, is not. Therefore, the following 8-bit binary addition (200+175) results in an error.

![Screen Shot 2020-07-03 at 10 29 13](https://user-images.githubusercontent.com/24994818/86482708-1c4e9c00-bd18-11ea-9c96-f9019c9b890f.png)

Rememver that the result must have the same bit count as the sources, and in this case, the 8-bit unsigned binary result 01110111 equals 119, not 375.

When adding unsigned binary values, there is a simple way to determine if an arithmetic overflow has ocuured. **In unsigned binary addition, if a carry is produced from the column representing the MSBs thereby requiring another bit for the representation, an overflow has occured**.

**In 2's complement addition, there is a different method for determining when an arithmetic overflow has occured**. To beging with, remember that an arithmetic overflow occurs when the result falls outside the minimyum and maximum values of the representation. In the case of 2's complement representation, those limits are defined by Equations 3.3 and 3.4.

The only way that this can happen is if two numbers with the sam sign are added togther. It is impossible for the addition of two numbers with different signs to reuslt in a value outside of the range of 2's complement representation.

when two numbers of the same sign are added together, however, there is a simple way to determine if an error has occrued. **If the result of the addition has the opposite sign of the two numbers being added, then the result is in error**. In other words, if the addition of two positive resulted in a negative number, or if the addition of two negative numbers reslted in a positive number, there were not enough bits in the representation to hold the result. The xample below presents one possible case:

![Screen Shot 2020-07-03 at 10 45 05](https://user-images.githubusercontent.com/24994818/86483724-4608c280-bd1a-11ea-8f32-70b81d01b590.png)

If this had been done assuming unsigned notation, the result of 152 would have been fine because no carry was generated. From equation 3.4, however, we see that the largest value that 8-bit 2's complement representation can hold is 2^(8-)-1 = 127. Since 152 is greater than 127, it is outside the range of 8-bit 2`s complement representation. In 2's complement representation, the bit pattern 10011000 actually represents -104. 

# * [3.10 What's Next?]()

computers use different numeric representations depending on the application. For example, a person's weight may be stored as a 16-bit integer while their house address may be stored in BCD. At this point, five binary representations have been introduced (unsigned binary, signed magnitude, 2's complement, BCD, and floating-point), and hexadecimal representations has been presented as a quick means of writing binary values.

Computers, however, do more with numbers that simply represent them. In Chapter 4, logic gates, the components that computers use to manipulate binary signals, will be presented. They are the lowest-level of computer hardware what we will be examining. We will use them to beging constructing the more complex componets of the computer.


# * [Problems]()

# 4. [Chapter Four: Logic Functions and Gates]()

Representing numbers using transistors is one thing, but getting the computer to do something with those numbers is an entirely different matter. Digital circuitry is used to perform operations such as addition or multiplication, manage data, or execute programs. This chapter presents the circuitry that is the foundation of data manipulation and management within a computer.

# * [4.1 Logic Gate Basics]()

Unless you are an electrical engineer, an understanding of the operation of transistors is unnecessary. One level above the transistors, however, is a set of basic building blocks for digital circuitry. These building blocks are called **logic gates**, and it is at this level that we will begin our discussion.
A logic gate has the basic format shown below in Figure 4-1. It takes one or more binary signals as its inputs, and using a specific algorithm, outputs a single bit as a result. Each time the inputs change, the output bit changes in a predictable fashion.

![Screen Shot 2020-07-05 at 23 32 30](https://user-images.githubusercontent.com/24994818/86556104-f2fc5e80-bf17-11ea-8265-0f14e27da51c.png)

For example, the algorithm for a specific gate may cause a one to be output if an odd number of ones are present at the gate's input and a zero to be output if an even number of ones is present.
A number of standard gates exist, each one of which has a specific symbol that uniquely identifies its function. Figure 4-2 presents the symbols for the four primary types of gates that are used in digital circuit design.

![Screen Shot 2020-07-05 at 23 34 01](https://user-images.githubusercontent.com/24994818/86556123-05769800-bf18-11ea-94e8-f392068447aa.png)

# * [4.1.1 NOT Gate]()

Let's begin with the **NOT gate**. This logic gate, sometimes referred to as an **inverter**, is the only one in Figure 4-2 that has a single input. Its input goes into the left side of a triangle symbol while its output exits the gate through a small circle placed at the tip of the opposite corner. Note that it is the small circle that defines the operation of this gate, so it should not be left out.
The NOT gate is used to flip the value of a digital signal. In other words, it changes a logic 1 input to a logic 0 or it changes a logic 0 input to a logic 1. An example of an inverter might be the light detection circuit used to control the automatic headlights of a car. During the daylight hours, sunshine enters the light detector which is typically installed on the top surface of the car's dashboard. This acts as a logic 1 input. Since it is daytime, the headlights need to be turned off, a logic 0. When the sun goes down and no light enters the light detector, a logic 0, then the headlights must be turned on, a logic 1. Figure 4-3 shows the operation of the NOT gate.

![Screen Shot 2020-07-05 at 23 35 11](https://user-images.githubusercontent.com/24994818/86556188-2fc85580-bf18-11ea-95ec-066d23e171b4.png)

Note that with a single input, the NOT gate has only 2 possible states.

# * [4.1.2 AND Gate]()

The operation of the AND gate is such that its output is a logic 1 only if all of its inputs are logic 1. Otherwise the output is a logic 0. The AND gate in Figure 4-2 has only two inputs, but an AND gate may have as many inputs as the circuit requires. Regardless of the number of inputs, all inputs must be a logic 1 for the output to be a logic 1.

As an example, picture a lamp that is connected to a plug in the wall that is subsequently controlled by the light switch that is protected with a circuit breaker. In order for the lamp to be on (logic 1), the switch at the lamp must be on, the wall switch must be on, and the circuit breaker must be on. If any of the switches turns to off (logic 0), then the lamp will turn off. Another way to describe the operation of this circuit might be to say, "The lamp is on if and only if the lamp switch is on and the wall switch is on and the circuit breaker is on." This should give you a good idea of when an AND gate is used; just look for the use of the word "and" when describing the operation of the circuit. Figure 4-4 shows all 22 = 4 states for a two-input AND gate.

![Screen Shot 2020-07-05 at 23 35 56](https://user-images.githubusercontent.com/24994818/86556219-4c648d80-bf18-11ea-9c9f-d9e80ff08ebb.png)

# * [4.1.3 OR Gate]()

An **OR gate** outputs a logic 1 if **any** of its inputs are a logic 1. An OR gate only outputs a logic 0 if all of its inputs are logic 0. The OR gate in Figure 4-2 has only two inputs, but just like the AND gate, an OR gate may have as many inputs as the circuit requires. Regardless of the number of inputs, if any input is a logic 1, the output is a logic 1.
A common example of an OR gate circuit is a security system. Assume that a room is protected by a system that watches three inputs: a door open sensor, a glass break sensor, and a motion sensor. If none of these sensors detects a break-in condition, i.e., they all send a logic 0 to the OR gate, the alarm is off (logic 0). If any of the sensors detects a break-in, it will send a logic 1 to the OR gate which in turn will output a logic 1 indicating an alarm condition. It doesn't matter what the other sensors are reading, if any sensor sends a logic 1 to the gate, the alarm should be going off. Another way to describe the operation of this circuit might be to say, "The alarm goes off if the door opens or the glass breaks or motion is detected." Once again, the use of the word "or" suggests that this circuit should be implemented with an OR gate.
![Screen Shot 2020-07-05 at 23 36 56](https://user-images.githubusercontent.com/24994818/86556256-6e5e1000-bf18-11ea-8ff6-5aa2321532c5.png)

# * [4.1.4 Exclusive-OR (XOR) Gate]()

An Exclusive-OR gate is sometimes called a **parity checker**. Parity checkers count the number of ones being input to a circuit and output a logic 1 or 0 based on whether the number of ones is odd or even. The Exclusive-OR (XOR) gate counts the number of ones at its input and outputs a logic 1 for an odd count and a logic 0 for an even count.
A common application for XOR gates is in **error checking circuits**. If two digital signals are compared bit-by-bit, an error free condition means that a logic 0 will be compared to a logic 0 and a logic 1 will be compared with a logic 1. In both of these cases, there is an even number of logic 1's being input to the XOR gate. Therefore, as long as the XOR gate outputs a logic 0, there is no error.
If, however, an error has occurred, then one signal will be logic 1 and the other will be a logic 0. This odd number of logic 1's will cause the XOR gate to output a logic 1 indicating an error condition.
Just as with the AND and OR gates, the XOR gate may have two or more inputs. Figure 4-6 shows all four states for a two-input XOR.

![Screen Shot 2020-07-05 at 23 40 33](https://user-images.githubusercontent.com/24994818/86556426-efb5a280-bf18-11ea-93c9-fc9fcfdcad50.png)

These representations of logic gates can be an awkward way to describe the operation of a complex circuit. The next section will introduce an easier method for representing the operation of any digital circuit incorporating the NOT, AND, OR, and XOR gates.

# * [4.2 Truth Tables]()

The previous section described the operation of each logic gate with words. This method isn't efficient and is prone to misinterpretation. What we need is a method to show the output of a digital system based on each of the possible input patterns of ones and zeros.
A **truth table** serves this purpose by making a column for each of the inputs to a digital circuit and a column for the resulting output. A row is added for each of the possible patterns of ones and zeros that could be input to the circuit. For example, a circuit with three inputs, A, B, and C, would have 23 = 8 possible patterns of ones and zeros:

* A=0, B=0, C=0 A=0, B=1, C=0 A=1, B=0, C=0 A=1, B=1, C=0 
* A=0, B=0, C=1 A=0, B=1, C=1 A=1, B=0, C=1 A=1, B=1, C=1

This means that a truth table representing a circuit with three inputs would have 8 rows. Figure 4-7 presents a sample truth table for a digital circuit with three inputs, A, B, and C, and one output, X. Note that the output X doesn't represent anything in particular. It is just added to show how the output might appear in a truth table.

![Screen Shot 2020-07-05 at 23 43 38](https://user-images.githubusercontent.com/24994818/86556584-5cc93800-bf19-11ea-99bd-5537da11b95e.png)

For the rest of this book, the inputs to a digital circuit will be labeled with capital letters, A, B, C, etc., while the output will be labeled X.
For some, the hardest part of creating a truth table is being able to list all possible patterns of ones and zeros for the inputs. One thing that can help us is that we know that for n inputs, there must be 2^n different patterns of inputs. Therefore, if your truth table doesn't have exactly 2^n rows, then a pattern is either missing or one has been duplicated.
There is also a trick to deriving the combinations. Assume we need to build a truth table with four inputs, A, B, C, and D. Since 2^4 = 16, we know that there will be sixteen possible combinations of ones and zeros. For half of those combinations, A will equal zero, and for the other half, A will equal one.
When A equals zero, the remaining three inputs, B, C, and D, will go through every possible combination of ones and zeros for three inputs. Three inputs have 2^3 = 8 patterns, which coincidentally, is half of 16. For half of the 8 combinations, B will equal zero, and for the other half, B will equal one. Repeat this for C and then D.
This gives us a process to create a truth table for four inputs. Begin with the A column and list eight zeros followed by eight ones. Half of eight is four, so in the B column write four zeros followed by four ones in the rows where A equals zero, then write four zeros followed by four ones in the rows where A equals one. Half of four equals two, so the C column will have two zeros followed by two ones followed by two zeros then two ones and so on. The process should end with the last column having alternating ones and zeros. If done properly, the first row should have all zeros and the last row should have all ones.

![Screen Shot 2020-07-05 at 23 46 12](https://user-images.githubusercontent.com/24994818/86556714-ba5d8480-bf19-11ea-9076-6f9641c83191.png)

In addition to verifying that all combinations of ones and zeros have been listed, this method also provides a consistency between all truth tables in the way that their rows are organized.
Now let's use truth tables to describe the functions of the four basic logic gates beginning with the inverter. The inverter has one input and one output. Therefore, there is one column for inputs and one column for outputs. For single input, there are exactly two possible states: logic 1 and logic 0. Therefore, there will be two rows of data for the inverter truth table. That table is shown in Figure 4-9.

![Screen Shot 2020-07-05 at 23 46 55](https://user-images.githubusercontent.com/24994818/86556760-d2cd9f00-bf19-11ea-8f2f-30e471898c18.png)

Remember that an AND gate outputs a logic 1 **only** if all of its inputs are logic 1. The operation of a two-input AND gate can be represented with the truth table shown in Figure 4-10.

![Screen Shot 2020-07-05 at 23 47 38](https://user-images.githubusercontent.com/24994818/86556803-ec6ee680-bf19-11ea-97b9-5822a37ceebd.png)

The output of an OR gate is set to logic 1 if any of its inputs equal 1. The OR gate truth table is shown in Figure 4-11.

![Screen Shot 2020-07-05 at 23 48 05](https://user-images.githubusercontent.com/24994818/86556840-fbee2f80-bf19-11ea-981b-a63db9dc12eb.png)

The XOR gate's output is set to logic 1 if there are an odd number of ones being input to the circuit. Figure 4-12 below shows that for a two- input XOR gate, this occurs twice, once for A=0 and B=1 and once for A=1 and B=0.

![Screen Shot 2020-07-05 at 23 48 33](https://user-images.githubusercontent.com/24994818/86556868-0c9ea580-bf1a-11ea-93f3-13328e7b2d65.png)

In some cases, the output of a digital circuit can be known without knowing what all of the inputs are. The AND gate, for instance, outputs a zero if any of the inputs equal zero. It doesn't matter what the other inputs are. This can be represented in a truth table with a third symbol called a **"don't care"**. The **"don't care"**, written as an 'X' in one of the input columns, indicates that the output does not depend on this input.
Take for example a three-input AND gate. Inputs B and C can take on one of four different values when the input A=0: B=0 and C=0; B=0 and C=1; B=1 and C=0; and B=1 and C=1. Each of these cases has an output of X=0. This can be shown in the truth table by replacing the four rows where A=0 with one row: A=0, B=X, and C=X. Figure 4-13 shows the resulting truth table where **"don't cares"** are used to reduce the number of rows. In this example, the original eight-row truth table has been replaced with one having only 4 rows.

![Screen Shot 2020-07-05 at 23 49 02](https://user-images.githubusercontent.com/24994818/86556885-1de7b200-bf1a-11ea-8794-541d59c9271a.png)

A similar truth table can be made for the OR gate. In this case, if any input to an OR gate is one, the output is 1. The only time an OR gate outputs a 0 is when all of the inputs are set to 0.

# * [4.3 Timing Diagrams for Gates]()

The operation of a logic gate can also be represented with a timing diagram. Figures 4-14, 4-15, and 4-16 show the output that results from three binary input signals for an AND gate, OR gate, and XOR gate respectively. Remember that the AND gate outputs a one only if all its inputs equal one, the OR gate outputs a one if any input equals one, and the XOR gate outputs a one if an odd number of ones is present at the input. Use these rules to verify the outputs shown in the figures.

![Screen Shot 2020-07-05 at 23 52 17](https://user-images.githubusercontent.com/24994818/86557020-92baec00-bf1a-11ea-890e-ddbd5db9bedf.png)

# * [4.4 Combinational Logic]()

y themselves, logic gates are not very practical. Their power comes when you combine them to create **Combinational Logic. **Combinational Logic connects multiple logic gates by using the outputs from some of the gates as the inputs to others. Figure 4-17 presents a sample of **Combinational Logic.

![Screen Shot 2020-07-05 at 23 53 28](https://user-images.githubusercontent.com/24994818/86557083-bd0ca980-bf1a-11ea-8b99-67fe6611f07d.png)

In an earlier section, a security system was given as an example for an application of an OR gate: the alarm goes off if the door opens or the glass breaks or motion is detected. This circuit description is incomplete though; it doesn't take into account the fact that security systems can be armed or disarmed. This would extend our system description to: the alarm goes off if the system is armed and (the door opens or the glass breaks or motion is detected). The parentheses are added here to remove any ambiguity in the precedence of the logical operations. Figure 4-18 shows our new security circuit.

![Screen Shot 2020-07-05 at 23 53 52](https://user-images.githubusercontent.com/24994818/86557096-cac22f00-bf1a-11ea-8036-150ef07b05cc.png)

The operation of this circuit can also be represented with a truth table. Figure 4-19 shows how the four inputs, Door, Glass, Motion, and Armed, affect the output Alarm. Note that Alarm never goes high (logic 1) if the system is disarmed, i.e., Armed = logic 0. If the system is armed, Armed = logic 1, but none of the alarm inputs are set to a logic 1, then the alarm stays off. If, however, the system is armed and any one of the other inputs is a logic 1, then the Alarm goes to a logic 1.

![Screen Shot 2020-07-05 at 23 54 30](https://user-images.githubusercontent.com/24994818/86557139-e75e6700-bf1a-11ea-9a10-3dbcd248a10a.png)

We determined the pattern of ones and zeros for the output column of the truth table through an understanding of the operation of a security system. We could have also done this by examining the circuit itself. Starting at the output side of Figure 4-18 (the right side) the AND gate will output a one only if **both** inputs are one, i.e., the system is armed and the OR gate is outputting a one.
The next step is to see when the OR gate outputs a one. This happens when **any** of the inputs, Door, Glass, or Motion, equal one. From this information, we can determine the truth table. The output of our circuit is equal to one when Armed=1 AND when either Door OR Glass OR Motion equal 1. For all other input conditions, a zero should be in the output column.
There are three **Combinational Logic circuits that are so common that they are considered gates in themselves. By adding an inverter to the output of each of the three logic gates, AND, OR, and XOR, three new **Combinational Logic circuits are created. Figure 4-20 shows the new logic symbols.

![Screen Shot 2020-07-05 at 23 57 25](https://user-images.githubusercontent.com/24994818/86557275-4d4aee80-bf1b-11ea-83aa-dd20fd3c8dc4.png)

The NAND gate outputs a 1 if any input is a zero. Later in this book, it will be shown how this gate is in fact a very important gate in the design of digital circuitry. It has two important characteristics: (1) the transistor circuit that realizes the NAND gate is typically one of the fastest circuits and (2) every digital circuit can be realized with **Combinational Logic made entirely of NAND gates.
The NOR gate outputs a 1 **only** if all of the inputs are zero. The Exclusive-NOR gate outputs a 1 as an indication that an even number of ones is being input to the gate.
A similar method is used to represent inverted inputs. Instead of inserting the NOT gate symbol in a line going to the input of a gate, a circle can be placed at the gate's input to show that the signal is inverted before entering the gate. An example of this is shown in the circuit on the right in Figure 4-21.

# * [4.5 Truth Tables for Combinational Logic]()

Not all digital circuits lend themselves to quick conversion to a truth table. For example, input B in the digital circuit shown in Figure 4-22 passes through four logic gates before its effect is seen at the output.

![Screen Shot 2020-07-05 at 23 59 24](https://user-images.githubusercontent.com/24994818/86557383-913df380-bf1b-11ea-9395-574dd99b3823.png)

So how do we convert this circuit to a truth table? One method is to go through each pattern of ones and zeros at the input and fill in the resulting output in the truth table row by row. Figure 4-23 takes A=0, B=0, and C=0 through each gate to determine its corresponding output.

![Screen Shot 2020-07-06 at 0 00 28](https://user-images.githubusercontent.com/24994818/86557430-b6326680-bf1b-11ea-9fc9-2e40e7f5035f.png)

This process can be rather tedious, especially if there are more than three inputs to the combinational logic. Note that the bit pattern in Figure 4-23 represents only one row of a truth table with eight rows. Add another input and the truth table doubles in size to sixteen rows.

There is another way to determine the truth table. Notice that in Figure 4-23, we took the inputs through a sequence of steps passing it first through the inverter connected to the input B, then through the AND gate, then through the OR gate, and lastly through the inverter connected to the output of the OR gate. These steps are labeled (a), (b), (c), and (d) in Figure 4-24.

![Screen Shot 2020-07-06 at 0 01 27](https://user-images.githubusercontent.com/24994818/86557473-d95d1600-bf1b-11ea-92d7-07c10397b510.png)

If we apply those same steps to the individual columns of a truth table instead of using the schematic, the process becomes more orderly. Begin by creating the input columns of the truth table listing all of the possible combinations of ones and zeros for the inputs. In the case of our sample circuit, that gives us a truth table with eight rows.

![Screen Shot 2020-07-06 at 0 02 07](https://user-images.githubusercontent.com/24994818/86557501-f1349a00-bf1b-11ea-9fcf-be056fe56591.png)

Next, add a column for each layer of logic. Going back to Figure 4- 24, we begin by making a column representing the (a) step. Since (a) represents the output of an inverter that has B as its input, fill the (a) column with the opposite or inverse of each condition in the B column.

![Screen Shot 2020-07-06 at 0 02 40](https://user-images.githubusercontent.com/24994818/86557534-06a9c400-bf1c-11ea-8459-f0f566d53d0c.png)

Next, step (b) is the output of an AND gate that takes as its inputs step (a) and input A. Add another column for step (b) and fill it with the AND of columns A and (a).

![Screen Shot 2020-07-06 at 0 03 16](https://user-images.githubusercontent.com/24994818/86557553-1a552a80-bf1c-11ea-9759-fb9cb0d50413.png)

Step (c) is the output from the OR gate that takes as its inputs step (b) and the input C. Add another column for (c) and fill it with the OR of column C and column (b). This is shown in Figure 4-28.
Last of all, Figure 4-29 shows the final output is the inverse of the output of the OR gate of step (c). Make a final column and fill it with the inverse of column (c). This will be the final output column for the truth table.

![Screen Shot 2020-07-06 at 0 03 45](https://user-images.githubusercontent.com/24994818/86557568-2b9e3700-bf1c-11ea-95bf-4bc7162a6230.png)

This can be done with any **combinational logic circuit. Begin by creating a table from the list of combinations of ones and zeros that are possible at the inputs. Next, determine the order of gates that the signals come to as they pass from the input to the output. As each set of signals passes through a gate, create another column in the truth table for the output of that gate. The final column should be the output of your **combinational logic circuit

# * [4.6 What's Next?]()

The introduction of logic operations and logic gates opens up the field of computer design. Topics ranging from the mathematical circuitry inside the processor to the creation and delivery of an Ethernet message will no longer remain abstract concepts.

Chapter 5 presents a mathematical-like method for representing logic circuits along with some techniques to manipulate them for faster performance or a lower chip count. These tools can then be used to effectively design the components of a computer system.

# * [Problems]()

# 5. [Chapter Five: Boolean Algebra]()

At this point in our study of digital circuits, we have two methods for representing combinational logic: schematics and truth tables.

![Screen Shot 2020-07-06 at 0 14 02](https://user-images.githubusercontent.com/24994818/86557993-a0be3c00-bf1d-11ea-8162-bf12f67aff19.png)

These two methods are inadequate for a number of reasons:

• Both schematics and truth tables take too much space to describe the operation of complex circuits with numerous inputs.
• The truth table "hides" circuit information.
• The schematic diagram is difficult to use when trying to determine
output values for each input combination.

To overcome these problems, a discipline much like algebra is practiced that uses expressions to describe digital circuitry. These expressions, which are called **boolean expressions*, use the input variable names, A, B, C, etc., and combine them using symbols representing the AND, OR, and NOT gates. These boolean expressions can be used to describe or evaluate the output of a circuit.
There is an additional benefit. Just like algebra, a set of rules exist that when applied to boolean expressions can dramatically simplify them. A simpler expression that produces the same output can be realized with fewer logic gates. A lower gate count results in cheaper circuitry, smaller circuit boards, and lower power consumption.
If your software uses binary logic, the logic can be represented with boolean expressions. Applying the rules of simplification will make the software run faster or allow it to use less memory.
The next section describes the representation of the three primary logic functions, NOT, AND, and OR, and how to convert combinational logic to a boolean expression.

# * [5.1 Need for Boolean Expressions]()

Analogous behavior can be shown between boolean algebra and mathematical algebra, and as a result, similar symbols and syntax can be used. For example, the following expressions hold true in math.


0·0=0 0·1=0 1·0=0 1·1=1

This looks like the AND function allowing an analogy to be drawn between the mathematical multiply and the boolean AND functions. Therefore, in boolean algebra, A AND'ed with B is written A · B.

![Screen Shot 2020-07-06 at 0 16 06](https://user-images.githubusercontent.com/24994818/86558054-e5e26e00-bf1d-11ea-9deb-6adc44e52aa2.png)

Mathematical addition has a similar parallel in boolean algebra, although it is not quite as flawless. The following four mathematical expressions hold true for addition.

0+0=0 0+1=1 1+0=1 1+1=2

The first three operations match the OR function, and if the last operation is viewed as having a non-zero result instead of the decimal result of two, it too can be viewed as operating similar to the OR function. Therefore, the boolean OR function is analogous to the mathematical function of addition.

![Screen Shot 2020-07-06 at 0 16 50](https://user-images.githubusercontent.com/24994818/86558093-001c4c00-bf1e-11ea-854b-88ca2c425cfc.png)

An analogy cannot be made between the boolean NOT and any mathematical operation. Later in this chapter we will see how the NOT function, unlike AND and OR, requires its own special theorems for algebraic manipulation. The NOT is represented with a bar across the inverted element.

![Screen Shot 2020-07-06 at 0 17 13](https://user-images.githubusercontent.com/24994818/86558109-0dd1d180-bf1e-11ea-866c-8909f5f29ead.png)

The NOT operation may be used to invert the result of a larger expression. For example, the NAND function which places an inverter at the output of an AND gate is written as:

![Screen Shot 2020-07-06 at 0 17 58](https://user-images.githubusercontent.com/24994818/86558143-280baf80-bf1e-11ea-8762-94862a6d45de.png)

Since the bar goes across A · B, the NOT is performed after the AND. Let's begin with some simple examples. Can you determine the
output of the boolean expression 1 + 0 + 1? Since the plus-sign represents the OR circuit, the expression represents 1 or 0 or 1.

![Screen Shot 2020-07-06 at 0 18 25](https://user-images.githubusercontent.com/24994818/86558167-38238f00-bf1e-11ea-87bd-ef6462f6a7a2.png)

Since an OR-gate outputs a 1 if any of its inputs equal 1, then 1 + 0 + 1 = 1.

The two-input XOR operation is represented using the symbol ⊕, but it can also be represented using a boolean expression. Basically, the two-input XOR equals one if A = 0 and B = 1 or if A = 1 and B = 0. This gives us the following expression.

![Screen Shot 2020-07-06 at 0 19 02](https://user-images.githubusercontent.com/24994818/86558189-4e314f80-bf1e-11ea-9a87-0ac16875e87f.png)

The next section shows how the boolean operators ·, +, ⊕, and the
NOT bar may be combined to represent complex combinational logic.

# * [5.2 Symbols of Boolean Algebra]()

Just as mathematical algebra combines multiplication and addition to create complex expressions, boolean algebra combines AND, OR, and NOT functions to represent complex combinational logic. Our experience with algebra allows us to understand the expression
Y = X · (X +5) + 3. The decimal value 5 is added to a copy of X, the result of which is then multiplied by a second copy of X. Lastly, a decimal 3 is added and the final result is assigned to Y.
This example shows us two things. First, each mathematical operation has a priority, e.g., multiplication is performed before addition. This priority is referred to as precedence. Second, variables such X can appear multiple times in an expression, each appearance representing the current value of X.
Boolean algebra allows for the same operation. Take for example the circuit shown in Figure 5-6.

![Screen Shot 2020-07-06 at 0 20 31](https://user-images.githubusercontent.com/24994818/86558269-859ffc00-bf1e-11ea-8c2d-182e278a3340.png)

In Chapter 4, we determined the truth table for this circuit by taking the input signals A, B, and C from left to right through each gate. As shown in Figure 5-7, we can do the same thing to determine the boolean expression.
Notice the use of parenthesis in step c. Just as in mathematical algebra, parenthesis can be used to force the order in which operations are taken. In the absence of parenthesis, however, the AND, OR, and NOT functions have an order of precedence.

![Screen Shot 2020-07-06 at 0 21 16](https://user-images.githubusercontent.com/24994818/86558315-a8321500-bf1e-11ea-8491-3f989f1c5a9b.png)

To begin with, AND takes precedence over OR unless overridden by parenthesis. NOT is a special case in that it can act like a set of parenthesis. If the bar indicating the NOT function spans a single variable, it takes precedence over AND and OR. If, however, the NOT bar spans an expression, the expression beneath the bar must be evaluated before the NOT is taken. Figure 5-8 presents two examples of handling precedence with the NOT function.

![Screen Shot 2020-07-06 at 0 22 05](https://user-images.githubusercontent.com/24994818/86558354-bbdd7b80-bf1e-11ea-9f89-6afdf521eed3.png)

Understanding this is vital because unlike the mathematical inverse, the two expressions below are not equivalent.

![Screen Shot 2020-07-06 at 0 22 34](https://user-images.githubusercontent.com/24994818/86558386-cdbf1e80-bf1e-11ea-924c-470abab0e012.png)

Let's do an example addressing precedence with a more complex boolean expression. Using parenthesis and the order of precedence, the boolean expression below has a single interpretation.

![Screen Shot 2020-07-06 at 0 23 04](https://user-images.githubusercontent.com/24994818/86558407-de6f9480-bf1e-11ea-9745-0ebe5fba869a.png)

The following steps show the order to evaluate the above expression.

1. OR B with C because the operation is contained under a single NOT bar and is contained within the lowest set of parenthesis
2. Invert the result of step 1 because NOT takes precedence over OR
3. OR A with the result of step 2 because of the parenthesis
4. Invert result of step 3
5. AND A and D because AND takes precedence over OR
6. OR the results of steps 4 and 5

We can use this order of operations to convert the expression to its
schematic representation. By starting with a list of inputs to the circuit, then passing each input through the correct gates, we can develop the circuit. Figure 5-9 does just this for the previous boolean expression. We list the inputs for the expression, A, B, C, and D, on the left side of the figure. These inputs are then passed through the gates using the same order as the steps shown above. The number inside each gate of the figure corresponds to the order of the steps.

![Screen Shot 2020-07-06 at 0 23 59](https://user-images.githubusercontent.com/24994818/86558465-ff37ea00-bf1e-11ea-9e74-f519f9882cc9.png)

The following sections show how boolean expressions can be used to modify combinational logic in order to reduce complexity or otherwise modify its structure.

# * [5.3 Boolean Expressions of Combinational Logic]()

The manipulation of algebraic expressions is based on fundamental laws. Some of these laws extend to the manipulation of boolean expressions. For example, the commutative law of algebra which states that the result of an operation is the same regardless of the order of operands holds true for boolean algebra too. This is shown for the OR function applied to two variables in the truth tables of Figure 5-10.

![Screen Shot 2020-07-06 at 0 24 50](https://user-images.githubusercontent.com/24994818/86558500-1d9de580-bf1f-11ea-8c36-cc941fbd1337.png)

Not only does Figure 5-10 show how the commutative law applies to the OR function, it also shows how truth tables can be used in boolean algebra to prove laws and rules. If a rule states that two boolean expressions are equal, then by developing the truth table for each expression and showing that the output is equal for all combinations of ones and zeros at the input, then the rule is proven true.
Below, the three fundamental laws of boolean algebra are given along with examples.

**Commutative Law**: The results of the boolean operations AND and OR are the same regardless of the order of their operands.

* A+B=B+A 
* A·B=B·A

**Associative Law**: The results of the boolean operations AND and OR with three or more operands are the same regardless of which pair of elements are operated on first.

* A + (B + C) = (A + B) + C 
* A · (B · C) = (A · B) · C

**Distributive Law**: The AND'ing of an operand with an OR expression is equivalent to OR'ing the results of an AND between the first operand and each operand within the OR expression.

A · (B + C) = A · B + A · C

The next section uses truth tables and laws to prove twelve rules of boolean algebra.

# * [5.4 Laws of Boolean Algebra]()
# 	* [5.5 Rules of Boolean Algebra]()
# * [5.5.1 NOT Rule]()
# * [5.5.2 OR Rules]()
# * [5.5.3 AND Rules]()
# * [5.5.4 XOR Rules]()
# * [5.5.5 Derivation of Other Rules]()
# * [5.6 Simplification]()
# * [5.7 DeMorgan's Theorem]()
# * [5.8 What's Next?]()
# * [Problems]()

# 6. [Chapter Six: Standard Boolean Expression Formats]()
# * [6.1 Sum-of-Products]()
# * [6.2 Converting an SOP Expression to a Truth Table]()
# * [6.3 Converting a Truth Table to an SOP Expression]()
# * [6.4 Product-of-Sums]()
# * [6.5 Converting POS to Truth Table]()
# * [6.6 Converting a Truth Table to a POS Expression]()
# * [6.7 NAND-NAND Logic]()
# * [6.8 What's Next?]()
# * [Problems]()

# 7. [Chapter Seven: Karnaugh Maps]()
# * [7.1 The Karnaugh Map]()
# * [7.2 Using Karnaugh Maps]()
# * [7.3 "Don't Care" Conditions in a Karnaugh Map]()
# * [7.4 What's Next?]()
# * [Problems]()

# 8. [Chapter Eight: Combinational Logic Applications]()
# * [8.1 Adders]()
# * [8.2 Seven-Segment Displays]()
# * [8.3 Active-Low Signals]()
# * [8.4 Decoders]()
# * [8.5 Multiplexers]()
# * [8.6 Demultiplexers]()
# * [8.7 Integrated Circuits]()
# * [8.8 What's Next?]()
# * [Problems]()

# 9. [Chapter Nine: Binary Operation Applications]()
# * [9.1 Bitwise Operations]()
# * [9.2 Comparing Bits with XOR]()
# * [9.3 Parity]()
# * [9.4 Checksum]()
# * [9.5 Cyclic Redundancy Check]()
# * [9.5.1 CRC Process]()
# * [9.5.2 CRC Implementation]()
# * [9.6 Hamming Code]()
# * [9.7 What's Next?]()
# * [Problems]()

# 10. [Chapter Ten: Memory Cells]()
# * [10.1 New Truth Table Symbols]()
# * [10.1.1 Edges/Transitions]()
# * [10.1.2 Previously Stored Values]()
# * [10.1.3 Undefined Values]()
# * [10.2 The S-R Latch]()
# * [10.3 The D Latch]()
# * [10.4 Divide-By-Two Circuit]()
# * [10.5 Counter]()
# * [10.6 Parallel Data Output]()
# * [10.7 What's Next?]()
# * [Problems]()

# 11.[ Chapter Eleven: State Machines]()
# * [11.1 Introduction to State Machines]()
# * [11.1.1 States]()
# * [11.1.2 State Diagrams]()
# * [11.1.3 Errors in State Diagrams]()
# * [11.1.4 Basic Circuit Organization]()
# * [11.2 State Machine Design Process]()
# * [11.3 Another State Machine Design: Pattern Detection]()
# * [11.4 Mealy Versus Moore State Machines]()
# * [11.5 What's Next?]()
# * [Problems]()

12. [Chapter Twelve: Memory Organization]()
# * [12.1 Early Memory]()
# * [12.2 Organization of Memory Device]()
# * [12.3 Interfacing Memory to a Processor]()
# * [12.3.1 Buses]()
# * [12.3.2 Memory Maps]()
# * [12.3.3 Address Decoding]()
# * [12.3.4 Chip Select Hardware]()
# * [12.4 Memory Mapped Input/Output]()
# * [12.5 Memory Terminology]()
# * [12.5.1 Random Access Memory]()
# * [12.5.2 Read Only Memory]()
# * [12.5.3 Static RAM versus Dynamic RAM]()
# * [12.5.4 Types of DRAM and Their Timing]()
# * [12.5.5 Asynchronous vs. Synchronous Memory]()
# * [12.6 What's Next?]()
# * [Problems]()

# 13.[ Chapter Thirteen: Memory Hierarchy]()
# * [13.1 Characteristics of the Memory Hierarchy]()
# * [13.2 Physical Characteristics of a Hard Drive]()
# * [13.2.1 Hard Drive Read/Write Head]()
# * [13.2.2 Data Encoding]()
# * [13.2.3 Hard Drive Access Time]()
# * [13.2.4 S.M.A.R.T.]()
# * [13.3 Organization of Data on a Hard Drive]()
# * [13.4 Cache RAM]()
# * [13.4.1 Cache Organization]()
# * [13.4.2 Dividing Memory into Blocks]()
# * [13.4.3 Cache Operation]()
# * [13.4.4 Cache Characteristics]()
# * [13.4.5 Cache Mapping Functions]()
# * [13.4.6 Cache Write Policy]()
# * [13.5 Registers]()
# * [13.6 What's Next?]()
# * [Problems]()

# 14.[ Chapter Fourteen: Serial Protocol Basics]()
# * [14.1 OSI Seven-Layer Network Model]()
# * [14.2 Serial versus Parallel Data Transmission]()
# * [14.3 Anatomy of a Frame or Packet]()
# * [14.4 Sample Protocol: IEEE 802.3 Ethernet]()
# * [14.5 Sample Protocol: Internet Protocol]()
# * [14.6 Sample Protocol: Transmission Control Protocol]()
# * [14.7 Dissecting a Frame]()
# * [14.8 Additional Resources]()
# * [14.9 What's Next?]()
# * [Problems]()

# 15.[ Chapter Fifteen: Introduction to Processor Architecture]()
# * [15.1 Organization versus Architecture]()
# * [15.2 Components]()
# * [15.2.1 Bus]()
# * [15.2.2 Registers]()
# * [15.2.3 Flags]()
# * [15.2.4 Buffers]()
# * [15.2.5 The Stack]()
# * [15.2.6 I/O Ports]()
# * [15.3 Processor Level]()
# * [15.4 CPU Level]()
# * [15.5 Simple Example of CPU Operation]()
# * [15.6 Assembly and Machine Language]()
# * [15.7 Big-Endian/Little-Endian]()
# * [15.8 Pipelined Architectures]()
# * [15.9 Passing Data To and From Peripherals]()
# * [15.9.1 Memory-Mapped I/O]()
# * [15.9.2 Polling]()
# * [15.9.3 Interrupts]()
# * [15.9.4 Direct Memory Access]()
# * [15.9.5 I/O Channels and Processors]()
# * [15.10 What's Next?]()
# * [Problems]()

# 16.[ Chapter Sixteen: Intel 80x86 Base Architecture]()
# * [16.1 Why Study the 80x86?]()
# * [16.2 Execution Unit]()
# * [16.2.1 General Purpose Registers]()
# * [16.2.2 Address Registers]()
# * [16.2.3 Flags]()
# * [16.2.4 Internal Buses]()
# * [16.3 Bus Interface Unit]()
# * [16.3.1 Segment Addressing]()
# * [16.3.2 Instruction Queue]()
# * [16.4 Memory versus I/O Ports]()
# * [16.5 What's Next?]()
# * [Problems]()

# 17.[ Chapter Seventeen: Intel 80x86 Assembly Language]()
# * [17.1 Assemblers versus Compilers]()
# * [17.2 Components of a Line of Assembly Language]()
# * [17.3 Assembly Language Directives]()
# * [17.3.1 SEGMENT Directive]()
# * [17.3.2 .MODEL, .STACK, .DATA, and .CODE Directives . 380 ]()
# * [17.3.3 PROC Directive]()
# * [17.3.4 END Directive]()
# * [17.3.5 Data Definition Directives]()
# * [17.3.6 EQU Directive]()
# * [17.4 80x86 Opcodes]()
# * [17.4.1 Data Transfer]()
# * [17.4.2 Data Manipulation]()
# * [17.4.3 Program Control]()
# * [17.4.4 Special Operations]()
# * [17.5 Addressing Modes]()
# * [17.5.1 Register Addressing]()
# * [17.5.2 Immediate Addressing]()
# * [17.5.3 Pointer Addressing]()
# * [17.6 Sample 80x86 Assembly Language Programs]()
# * [17.7 Additional 80x86 Programming Resources]()
# * [17.8 What's Next?]()
# * [Problems]()

